# ðŸ§  Ollama Function-Calling Chatbot (with Arxiv Tool)

A locally hosted chatbot powered by [Ollama](https://ollama.com), capable of **function calling** to fetch research papers from [arXiv.org](https://arxiv.org/). Uses `gemma:7b` as the LLM, `FastAPI` for serving tools, and Docker for local orchestration.

---

## âš™ï¸ Project Structure

```
.
â””â”€â”€ ollama_function_calling_without_mcp
    â”œâ”€â”€ app_server
    â”‚Â Â  â”œâ”€â”€ app_server.py
    â”‚Â Â  â”œâ”€â”€ Dockerfile
    â”‚Â Â  â”œâ”€â”€ requirements.txt
    â”‚Â Â  â””â”€â”€ tools.py
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ host
    â”‚Â Â  â”œâ”€â”€ cli_chatbot.py
    â”‚Â Â  â”œâ”€â”€ Dockerfile
    â”‚Â Â  â”œâ”€â”€ llama_client.py
    â”‚Â Â  â””â”€â”€ requirements.txt
    â””â”€â”€ llm_server
        â””â”€â”€ instructions.md
```

---

## What are we going to build?


![alt text](./images/chatbot_system_design.png)

![alt text](./images/sequence_diagram_ollama_chatbot.png)


---

## ðŸš€ Getting Started

### âœ… Step 1: Start the Ollama Server Manually

Make sure you have Ollama installed and run the LLM (e.g. `gemma:7b`) outside Docker.

```bash
ollama pull gemma:7b
ollama run gemma:7b
```

> This starts the Ollama LLM server at http://localhost:11434.

---

### âœ… Step 2: Start the Tool Server and Chatbot Client Containers
From the project root:

1. Start the FastAPI app_server in detached mode

```bash
docker compose up -d --build app_server
```

2. Start the chatbot client (host) in interactive mode

```bash
docker compose run --rm host
```
This gives you a live CLI interface like:

```bash
You: Show me papers on retrieval augmented generation
Bot: ðŸ“„ "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" â€” [arxiv.org/...]
```

---


### Optional Step: ðŸ”Ž Health Checks

#### âœ… Ollama Health Check
The chatbot will automatically check if Ollama is running using /api/tags.

If not reachable:

âŒ Ollama server not reachable. Please start it with:

    ollama run gemma:7b

#### âœ… Tool Server Health Check
You can verify that the app server is up:

```bash
curl http://localhost:8000/health
# {"status": "ok"}
```

---


### ðŸ“¦ Requirements

```
Ollama
Docker + Docker Compose
Internet access (for arXiv API)
```

---

### ðŸ§© Supported Tools

ðŸ” Arxiv Search
Function: arxiv_search(q: str, max_results: int = 5)

Example Prompt:

â€œSearch latest papers on reinforcement learning in arXiv.â€

---

## How does the Communication happen between Client and Servers

### ðŸ”¸ 1. Client (CLI) â†’ Ollama Server
- âœ… Transport: streamable HTTP POST to /api/chat
- âœ… Protocol: JSON-over-HTTP (streamed line-by-line)
- âœ… Pattern: Client sends a message; receives streamed JSON chunks


### ðŸ”¸ 2. Client (CLI) â†’ Tool Server (app_server)
- âœ… Transport: Standard HTTP GET request (/arxiv?q=...)
- âœ… Protocol: JSON response, synchronous
- âœ… Pattern: Traditional HTTP API call

### 3. Client is the orchestrator
The CLI (cli_chatbot.py) acts as:

- Router: handles the LLM response
- Function-caller: detects function_call and delegates to tools
- Memory holder: maintains history in context

---

---

## Result

### Version 1: Without enabling debugging

- Since we have used a quantized model for the web

```bash
WARN[0000] /path/to/ollama_function_calling_without_mcp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Creating 1/1
 âœ” Container ollama_function_calling_without_mcp-app_server-1  Running                                                                                                                                      0.0s 

ðŸ’¬ Chat started. Type 'exit' or 'quit' to stop.

You: Hi
Bot: Hi there! ðŸ‘‹ 

 
You: Search papers that talk about or use "Behaviour Link Graph"
Bot: **Relevant papers:**

**1. Behavior-Centric Link Prediction in Behavioral Link Graphs**
* Authors: Xiangnan Wang, Hongyuan Zha, et al.
* Publication: KDD, 2018
* Abstract: This paper proposes a link prediction model based on user behavior in behavioral link graphs.

**2. Learning Representation for Behavioral Link Graphs**
* Authors: Mengnan Wang, Hongyuan Zha, et al.
* Publication: AAAI, 2019
* Abstract: This paper explores representation learning techniques for behavioral link graphs, focusing on capturing the temporal dynamics of user behavior.

**3. Behavioral Link Graph Embedding for Recommendation**
* Authors: Zijian He, Hongyuan Zha, et al.
* Publication: SIGIR, 2019
* Abstract: This paper presents a recommendation system based on behavioral link graph embedding, which captures the relationships between items and users.

**4. A Survey on Behavioral Link Graphs: Representation, Inference, and Applications**
* Authors: Chenhao Tan, Hongyuan Zha, et al.
* Publication: arXiv, 2020
* Abstract: This paper provides a comprehensive overview of behavioral link graphs, covering representation models, inference algorithms, and applications.

**5. Learning Topic-Sensitive Behavioral Link Graphs for Recommendation**
* Authors: Xingxing Zhang, Hongyuan Zha, et al.
* Publication: AAAI, 2021
* Abstract: This paper proposes a method for learning topic-sensitive behavioral link graphs, which captures the relationships between items and topics.

**Additional resources:**

* **GitHub repository:** https://github.com/facebookresearch/BLG
* **Google Scholar search:** https://scholar.google.com/scholar?q=behavioral+link+graphs

**Keywords:**

* Behavioral Link Graphs (BLGs)
* Link prediction
* Representation learning
* Recommendation systems
* Social network analysis

You: quit    
ðŸ‘‹ Exiting chat. Goodbye!

```

---

## Issue I encountered

### â“What Happens When stream=False (default)?

âœ‰ï¸ Code Before:

```python
response = requests.post(
    "http://host.docker.internal:11434/api/chat",
    json={"model": "gemma:7b", "messages": messages}
    # stream=False is default
)
response.json()
```

ðŸ“¦ Protocol Used:
- Transport: Regular HTTP POST
- Response: Ollama buffers the entire response into a single JSON object
- Protocol style: blocking HTTP, not streaming

You get something like:

```json
{
  "message": {
    "role": "assistant",
    "content": "Hello there!"
  }
}
```

âš ï¸ Problem:

- If Ollama returns a streaming NDJSON response by default, requests still buffers it all â€” and the resulting response body may look like:

```json
{"message": {"role": "assistant", "content": "Hello"}}
{"message": {"role": "assistant", "content": " there!"}}
```
This is not valid JSON, and calling response.json() fails with:

JSONDecodeError: Extra data: line 2 column 1 (char 123)
Because you're trying to decode a multi-document stream as a single JSON object â€” which is invalid.

âœ… What Changes with stream=True?

âœ‰ï¸ Code After:

```python
response = requests.post(..., stream=True)

```


ðŸ“¦ Protocol Used:
- Still HTTP POST, but now:
- Youâ€™re telling requests not to read the full body at once
- You read it line-by-line using response.iter_lines()
- Each line is a valid JSON object â€” NDJSON protocol



---

## âš ï¸ Disclaimer

> While all the above code has been verified and tested to function correctly, please note that parts of this were generated via vibe coding â€” assisted by AI tools to accelerate experimentation. Review and adapt to your own development standards where needed.