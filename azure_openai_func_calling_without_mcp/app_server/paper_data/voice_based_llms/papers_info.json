{
  "2410.17196v3": {
    "title": "VoiceBench: Benchmarking LLM-Based Voice Assistants",
    "authors": [
      "Yiming Chen",
      "Xianghu Yue",
      "Chen Zhang",
      "Xiaoxue Gao",
      "Robby T. Tan",
      "Haizhou Li"
    ],
    "summary": "Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.",
    "pdf_url": "http://arxiv.org/pdf/2410.17196v3",
    "published": "2024-10-22"
  },
  "2409.15623v1": {
    "title": "Safe Guard: an LLM-agent for Real-time Voice-based Hate Speech Detection in Social Virtual Reality",
    "authors": [
      "Yiwen Xu",
      "Qinyang Hou",
      "Hongyu Wan",
      "Mirjana Prpa"
    ],
    "summary": "In this paper, we present Safe Guard, an LLM-agent for the detection of hate\nspeech in voice-based interactions in social VR (VRChat). Our system leverages\nOpen AI GPT and audio feature extraction for real-time voice interactions. We\ncontribute a system design and evaluation of the system that demonstrates the\ncapability of our approach in detecting hate speech, and reducing false\npositives compared to currently available approaches. Our results indicate the\npotential of LLM-based agents in creating safer virtual environments and set\nthe groundwork for further advancements in LLM-driven moderation approaches.",
    "pdf_url": "http://arxiv.org/pdf/2409.15623v1",
    "published": "2024-09-23"
  },
  "2309.13879v2": {
    "title": "User Interaction Patterns and Breakdowns in Conversing with LLM-Powered Voice Assistants",
    "authors": [
      "Amama Mahmood",
      "Junxiang Wang",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Chien-Ming Huang"
    ],
    "summary": "Conventional Voice Assistants (VAs) rely on traditional language models to\ndiscern user intent and respond to their queries, leading to interactions that\noften lack a broader contextual understanding, an area in which Large Language\nModels (LLMs) excel. However, current LLMs are largely designed for text-based\ninteractions, thus making it unclear how user interactions will evolve if their\nmodality is changed to voice. In this work, we investigate whether LLMs can\nenrich VA interactions via an exploratory study with participants (N=20) using\na ChatGPT-powered VA for three scenarios (medical self-diagnosis, creative\nplanning, and discussion) with varied constraints, stakes, and objectivity. We\nobserve that LLM-powered VA elicits richer interaction patterns that vary\nacross tasks, showing its versatility. Notably, LLMs absorb the majority of VA\nintent recognition failures. We additionally discuss the potential of\nharnessing LLMs for more resilient and fluid user-VA interactions and provide\ndesign guidelines for tailoring LLMs for voice assistance.",
    "pdf_url": "http://arxiv.org/pdf/2309.13879v2",
    "published": "2023-09-25"
  },
  "2505.17093v1": {
    "title": "Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech",
    "authors": [
      "Yejin Lee",
      "Jaehoon Kang",
      "Kyuhong Shim"
    ],
    "summary": "In this paper, we propose a novel framework to control voice style in\nprompt-based, controllable text-to-speech systems by leveraging textual\npersonas as voice style prompts. We present two persona rewriting strategies to\ntransform generic persona descriptions into speech-oriented prompts, enabling\nfine-grained manipulation of prosodic attributes such as pitch, emotion, and\nspeaking rate. Experimental results demonstrate that our methods enhance the\nnaturalness, clarity, and consistency of synthesized speech. Finally, we\nanalyze implicit social biases introduced by LLM-based rewriting, with a focus\non gender. We underscore voice style as a crucial factor for persona-driven AI\ndialogue systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17093v1",
    "published": "2025-05-21"
  },
  "2505.22251v2": {
    "title": "Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition",
    "authors": [
      "Yuan Tseng",
      "Titouan Parcollet",
      "Rogier van Dalen",
      "Shucong Zhang",
      "Sourav Bhattacharya"
    ],
    "summary": "Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\ncontamination impact, LLMs trained with/without contamination are compared. A\ncontaminated LLM is more likely to generate test sentences it has seen during\ntraining. Then, speech recognisers based on LLMs are compared. They show only\nsubtle error rate differences if the LLM is contaminated, but assign\nsignificantly higher probabilities to transcriptions seen during LLM training.\nResults show that LLM outputs can be biased by tiny amounts of data\ncontamination, highlighting the importance of evaluating LLM-based speech\nsystems with held-out data.",
    "pdf_url": "http://arxiv.org/pdf/2505.22251v2",
    "published": "2025-05-28"
  }
}